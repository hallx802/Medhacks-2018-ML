clear; clc; close all;
mlRound = 1;


myFolder = 'C:\Users\Faiz\Desktop\MedHacks2018';


% filePattern = fullfile(myFolder, '*.csv');
% emoFiles = dir(filePattern);
filename= "training.1600000.processed.noemoticon halved.csv";
data = readtable(filename,'TextType','string');
data.sentiment = categorical(data.sentiment);

f = figure;
f.Position(3) = 1.5*f.Position(3);

h = histogram(data.sentiment);
xlabel("Class")
ylabel("Frequency")
title("Class Distribution")

classCounts = h.BinCounts;
classNames = h.Categories;

idxLowCounts = classCounts < 700;
infrequentClasses = classNames(idxLowCounts)

idxInfrequent = ismember(data.sentiment,infrequentClasses);
data(idxInfrequent,:) = [];
data.sentiment = removecats(data.sentiment);

cvp = cvpartition(data.sentiment,'Holdout',0.1);
dataTrain = data(training(cvp),:);
dataTest = data(test(cvp),:);

textDataTrain = dataTrain.content;
textDataTest = dataTest.content;
YTrain = dataTrain.sentiment;
YTest = dataTest.sentiment;




textDataTrain = erasePunctuation(textDataTrain);
textDataTrain = lower(textDataTrain);
documentsTrain = tokenizedDocument(textDataTrain);



documentsTrain(1:5);

%% Machine Learning
% Solve a Pattern Recognition Problem with a Neural Network
% Script generated by Neural Pattern Recognition app
% Created 13-Feb-2018 15:13:06


embeddingDimension = 50;
embeddingEpochs = 25;
emb = trainWordEmbedding(documentsTrain, ...
    'Dimension',embeddingDimension, ...
    'NumEpochs',embeddingEpochs, ...
    'Verbose',0)

documentLengths = doclength(documentsTrain);
figure
histogram(documentLengths)
title("Document Lengths")
xlabel("Length")
ylabel("Number of Documents")
sequenceLength = 30;
documentsTruncatedTrain = docfun(@(words) words(1:min(sequenceLength,end)),documentsTrain);
XTrain = doc2sequence(emb,documentsTruncatedTrain);

for i = 1:numel(XTrain)
    XTrain{i} = leftPad(XTrain{i},sequenceLength);
end

inputSize = embeddingDimension;
outputSize = 180;
numClasses = numel(categories(YTrain));

layers = [ ...
    sequenceInputLayer(inputSize)
    lstmLayer(outputSize,'OutputMode','last')
    fullyConnectedLayer(numClasses)
    softmaxLayer
    classificationLayer]

options = trainingOptions('adam', ...
    'GradientThreshold',1, ...
    'InitialLearnRate',0.01, ...
    'Plots','training-progress', ...
    'Verbose',0);

net = trainNetwork(XTrain,YTrain,layers,options);

textDataTest = erasePunctuation(textDataTest);
textDataTest = lower(textDataTest);
documentsTest = tokenizedDocument(textDataTest);

documentsTruncatedTest = docfun(@(words) words(1:min(sequenceLength,end)),documentsTest);
XTest = doc2sequence(emb,documentsTruncatedTest);
for i=1:numel(XTest)
    XTest{i} = leftPad(XTest{i},sequenceLength);
end

YPred = classify(net,XTest);

accuracy = sum(YPred == YTest)/numel(YPred)

